{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e011b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install g2p_en\n",
    "#!pip install scipy\n",
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea25c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re \n",
    "from g2p_en import G2p\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33196a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t12.2022.04.28', 't12.2022.05.05', 't12.2022.05.17', 't12.2022.05.19', 't12.2022.05.24', 't12.2022.05.26', 't12.2022.06.02', 't12.2022.06.07', 't12.2022.06.14', 't12.2022.06.16', 't12.2022.06.21', 't12.2022.06.23', 't12.2022.06.28', 't12.2022.07.05', 't12.2022.07.14', 't12.2022.07.21', 't12.2022.07.27', 't12.2022.07.29', 't12.2022.08.02', 't12.2022.08.11', 't12.2022.08.13', 't12.2022.08.18', 't12.2022.08.23', 't12.2022.08.25']\n"
     ]
    }
   ],
   "source": [
    "root_directory = os.environ['DATA']\n",
    "os.sys.path.append(root_directory)\n",
    "\n",
    "datasets = '/willett2023/competitionData'\n",
    "sessionNames = os.listdir(root_directory + datasets + '/train/')\n",
    "sessionNames = [name.replace('.mat', '') for name in sessionNames]\n",
    "\n",
    "sessionNames.sort()\n",
    "print(sessionNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edb2175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2p = G2p()\n",
    "PHONE_DEF = [\n",
    "    'AA', 'AE', 'AH', 'AO', 'AW',\n",
    "    'AY', 'B',  'CH', 'D', 'DH',\n",
    "    'EH', 'ER', 'EY', 'F', 'G',\n",
    "    'HH', 'IH', 'IY', 'JH', 'K',\n",
    "    'L', 'M', 'N', 'NG', 'OW',\n",
    "    'OY', 'P', 'R', 'S', 'SH',\n",
    "    'T', 'TH', 'UH', 'UW', 'V',\n",
    "    'W', 'Y', 'Z', 'ZH'\n",
    "]\n",
    "PHONE_DEF_SIL = PHONE_DEF + ['SIL']\n",
    "\n",
    "def phoneToId(p):\n",
    "    return PHONE_DEF_SIL.index(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20398fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFeaturesAndNormalize(sessionPath):\n",
    "    \n",
    "    dat = scipy.io.loadmat(sessionPath)\n",
    "\n",
    "    input_features = []\n",
    "    transcriptions = []\n",
    "    frame_lens = []\n",
    "    block_means = []\n",
    "    block_stds = []\n",
    "    n_trials = dat['sentenceText'].shape[0]\n",
    "\n",
    "    #collect area 6v tx1 and spikePow features\n",
    "    for i in range(n_trials):    \n",
    "        #get time series of TX and spike power for this trial\n",
    "        #first 128 columns = area 6v only\n",
    "        features = np.concatenate([dat['tx1'][0,i][:,0:128], dat['spikePow'][0,i][:,0:128]], axis=1)\n",
    "\n",
    "        sentence_len = features.shape[0]\n",
    "        sentence = dat['sentenceText'][i].strip()\n",
    "\n",
    "        input_features.append(features)\n",
    "        transcriptions.append(sentence)\n",
    "        frame_lens.append(sentence_len)\n",
    "\n",
    "    #block-wise feature normalization\n",
    "    blockNums = np.squeeze(dat['blockIdx'])\n",
    "    blockList = np.unique(blockNums)\n",
    "    blocks = []\n",
    "    for b in range(len(blockList)):\n",
    "        sentIdx = np.argwhere(blockNums==blockList[b])\n",
    "        sentIdx = sentIdx[:,0].astype(np.int32)\n",
    "        blocks.append(sentIdx)\n",
    "\n",
    "    for b in range(len(blocks)):\n",
    "        feats = np.concatenate(input_features[blocks[b][0]:(blocks[b][-1]+1)], axis=0)\n",
    "        feats_mean = np.mean(feats, axis=0, keepdims=True)\n",
    "        feats_std = np.std(feats, axis=0, keepdims=True)\n",
    "        for i in blocks[b]:\n",
    "            input_features[i] = (input_features[i] - feats_mean) / (feats_std + 1e-8)\n",
    "\n",
    "    #convert to tfRecord file\n",
    "    session_data = {\n",
    "        'inputFeatures': input_features,\n",
    "        'transcriptions': transcriptions,\n",
    "        'frameLens': frame_lens\n",
    "    }\n",
    "\n",
    "    return session_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "588891db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_to_ascii(text):\n",
    "    return [ord(char) for char in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cf55fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(fileName):\n",
    "    session_data = loadFeaturesAndNormalize(fileName)\n",
    "        \n",
    "    allDat = []\n",
    "    trueSentences = []\n",
    "    seqElements = []\n",
    "    \n",
    "    for x in range(len(session_data['inputFeatures'])):\n",
    "        allDat.append(session_data['inputFeatures'][x])\n",
    "        \n",
    "        thisTranscription = str(session_data['transcriptions'][x]).strip()\n",
    "        thisTranscription = re.sub(r'[^a-zA-Z\\- \\']', '', thisTranscription)\n",
    "        thisTranscription = thisTranscription.replace('--', '').lower()\n",
    "        addInterWordSymbol = True\n",
    "\n",
    "        phonemes = []\n",
    "        for p in g2p(thisTranscription):\n",
    "            if addInterWordSymbol and p==' ':\n",
    "                phonemes.append('SIL')\n",
    "            p = re.sub(r'[0-9]', '', p)  # Remove stress\n",
    "            if re.match(r'[A-Z]+', p):  # Only keep phonemes\n",
    "                phonemes.append(p)\n",
    "\n",
    "        #add one SIL symbol at the end so there's one at the end of each word\n",
    "        if addInterWordSymbol:\n",
    "            phonemes.append('SIL')\n",
    "\n",
    "        seqLen = len(phonemes)\n",
    "        maxSeqLen = 500\n",
    "        seqClassIDs = np.zeros([maxSeqLen]).astype(np.int32)\n",
    "        seqClassIDs[0:seqLen] = [phoneToId(p) + 1 for p in phonemes] # + 1 to be consistent with the pipeline, the shifted on eval\n",
    "        seqElements.append(seqClassIDs)\n",
    "        paddedTranscription = np.zeros([maxSeqLen]).astype(np.int32)\n",
    "        paddedTranscription[0:len(thisTranscription)] = np.array(_convert_to_ascii(thisTranscription))\n",
    "        trueSentences.append(paddedTranscription)\n",
    "\n",
    "    newDataset = {}\n",
    "    newDataset['sentenceDat'] = allDat\n",
    "    newDataset['transcriptions'] = trueSentences\n",
    "    newDataset['phonemes'] = seqElements\n",
    "    \n",
    "    timeSeriesLens = []\n",
    "    phoneLens = []\n",
    "    for x in range(len(newDataset['sentenceDat'])):\n",
    "        timeSeriesLens.append(newDataset['sentenceDat'][x].shape[0])\n",
    "        \n",
    "        zeroIdx = np.argwhere(newDataset['phonemes'][x]==0)\n",
    "        phoneLens.append(zeroIdx[0,0])\n",
    "    \n",
    "    newDataset['timeSeriesLens'] = np.array(timeSeriesLens)\n",
    "    newDataset['phoneLens'] = np.array(phoneLens)\n",
    "    newDataset['phonePerTime'] = newDataset['phoneLens'].astype(np.float32) / newDataset['timeSeriesLens'].astype(np.float32)\n",
    "    return newDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "815eca0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [01:22<00:00,  3.43s/it]\n"
     ]
    }
   ],
   "source": [
    "trainDatasets = []\n",
    "testDatasets = []\n",
    "competitionDatasets = []\n",
    "\n",
    "dataDir = root_directory + datasets\n",
    "\n",
    "for dayIdx in tqdm(range(len(sessionNames))):\n",
    "    trainDataset = getDataset(dataDir + '/train/' + sessionNames[dayIdx] + '.mat')\n",
    "    testDataset = getDataset(dataDir + '/test/' + sessionNames[dayIdx] + '.mat')\n",
    "\n",
    "    trainDatasets.append(trainDataset)\n",
    "    testDatasets.append(testDataset)\n",
    "\n",
    "    if os.path.exists(dataDir + '/competitionHoldOut/' + sessionNames[dayIdx] + '.mat'):\n",
    "        dataset = getDataset(dataDir + '/competitionHoldOut/' + sessionNames[dayIdx] + '.mat')\n",
    "        competitionDatasets.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de32e23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 18, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "competitionDays = []\n",
    "for dayIdx in range(len(sessionNames)):\n",
    "    if os.path.exists(dataDir + '/competitionHoldOut/' + sessionNames[dayIdx] + '.mat'):\n",
    "        competitionDays.append(dayIdx)\n",
    "print(competitionDays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80014502",
   "metadata": {},
   "outputs": [],
   "source": [
    "allDatasets = {}\n",
    "allDatasets['train'] = trainDatasets\n",
    "allDatasets['test'] = testDatasets\n",
    "allDatasets['competition'] = competitionDatasets\n",
    "\n",
    "with open(root_directory + datasets + '/pytorchTFRecords.pkl', 'wb') as handle:\n",
    "    pickle.dump(allDatasets, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e084b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pnpl_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
