{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cd $HOME/speechBCI/NeuralDecoder && pip install --user -e .\n",
    "#!cd $HOME/speechBCI/LanguageModelDecoder/runtime/server/x86 && python setup.py install\n",
    "#!pip install causal-conv1d\n",
    "#!cd $HOME/mamba && pip install --user -e .\n",
    "#!cd $HOME/neural_seq_decoder && pip install --user -e .\n",
    "#!pip install pytorch-lightning\n",
    "#!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Script vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import sys\n",
    "\n",
    "from models.datasetLoaders import getDatasetLoaders\n",
    "from models.mamba_phoneme import MambaPhoneme\n",
    "from models.lightning_wrapper import LightningWrapper\n",
    "from mamba_ssm.models.config_mamba import MambaConfig\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "#from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from ipyfilechooser import FileChooser\n",
    "\n",
    "confPath = FileChooser(os.environ['DATA'] + '/willett2023/experiments')\n",
    "confPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confPath = confPath.selected\n",
    "\n",
    "# Load the configuration pickle file\n",
    "print(\"Loading configuration from: \", confPath, flush=True)\n",
    "args = pickle.load(open(confPath, 'rb'))\n",
    "\n",
    "ssm_cfg = {\n",
    "        'd_state'   : args[\"d_state\"],\n",
    "        'd_conv'    : args[\"d_conv\"],\n",
    "        'expand'    : args[\"expand\"],\n",
    "        'dt_rank'   : args[\"dt_rank\"],\n",
    "        'dt_min'    : args[\"dt_min\"],\n",
    "        'dt_max'    : args[\"dt_max\"],\n",
    "        'dt_init'   : args[\"dt_init\"],\n",
    "        'dt_scale'  : args[\"dt_scale\"],\n",
    "        'dt_init_floor' : args[\"dt_init_floor\"],\n",
    "        'conv_bias' : args[\"conv_bias\"],\n",
    "        'bias'      : args[\"bias\"],\n",
    "        'use_fast_path' : args[\"use_fast_path\"],  # Fused kernel options\n",
    "        }\n",
    "\n",
    "torch.manual_seed(args[\"seed\"])\n",
    "np.random.seed(args[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader, testLoader, loadedData = getDatasetLoaders(\n",
    "    args['dataset'], args['batchSize']\n",
    ")\n",
    "\n",
    "args['nDays'] = len(loadedData[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreModel = MambaPhoneme(\n",
    "    config=MambaConfig(\n",
    "        d_model=args['nInputFeatures'],\n",
    "        n_layer=args['nLayers'],\n",
    "        vocab_size=args['nClasses'],\n",
    "        ssm_cfg=ssm_cfg,\n",
    "        rms_norm=False,\n",
    "        residual_in_fp32=False,\n",
    "        fused_add_norm=False,\n",
    "    ),\n",
    "    device=args['device'],\n",
    "    dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coreModel.modelName)\n",
    "print('Number of parameters: ', sum(p.numel() for p in coreModel.parameters() if p.requires_grad))\n",
    "print('\\n--------------------\\n')\n",
    "print(coreModel)\n",
    "print('\\n--------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds and setup output directory\n",
    "timestamp = int(time.time())\n",
    "logsPath = args['baseDir'] + \"experiments/logs\"\n",
    "checkpointPath = args['experimentPath'] + \"/checkpoints/\" + args['modelName'] + \"_\" + str(timestamp)\n",
    "\n",
    "# Define the logger\n",
    "#logger = TensorBoardLogger(logsPath, name=coreModel.modelName)\n",
    "wandb_logger = WandbLogger(project='PNPL', name=args['modelName'], log_model='all', save_dir=logsPath)\n",
    "\n",
    "# Define the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=checkpointPath,\n",
    "    monitor='val_loss',\n",
    "    filename='{epoch:02d}-{val_loss:.2f}-{avg_val_cer:.2f}',\n",
    "    save_last=False,\n",
    "    save_top_k=2,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.00,\n",
    "    patience=20,\n",
    "    verbose=False,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint_callback, early_stop_callback]\n",
    "\n",
    "loss_ctc = torch.nn.CTCLoss(blank=0, reduction=\"mean\", zero_infinity=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    coreModel.parameters(),\n",
    "    lr=args[\"lrStart\"],\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=0.1,\n",
    "    weight_decay=args[\"l2_decay\"],\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=1.0,\n",
    "    end_factor=args[\"lrEnd\"] / args[\"lrStart\"],\n",
    "    total_iters=args[\"nEpochs\"],\n",
    ")\n",
    "\n",
    "# Training\n",
    "model = LightningWrapper(coreModel, loss_ctc, optimizer, args, scheduler, willetts_preprocessing_pipeline = args['pppipeline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=args[\"nEpochs\"],\n",
    "    log_every_n_steps=100,\n",
    "    check_val_every_n_epoch=1,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=callbacks,\n",
    "    enable_progress_bar=False\n",
    ")\n",
    "\n",
    "trainer.fit(model, trainLoader, val_dataloaders=testLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) > 1:\n",
    "        confPath = sys.argv[1]\n",
    "        train(confPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
