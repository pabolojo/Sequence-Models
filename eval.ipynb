{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ipyfilechooser\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from neural_decoder.dataset import SpeechDataset\n",
    "\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "from neuralDecoder.utils.lmDecoderUtils import lm_decode\n",
    "from neuralDecoder.utils.lmDecoderUtils import rearrange_speech_logits\n",
    "from neural_decoder.neural_decoder_trainer import getDatasetLoaders\n",
    "from neural_decoder.neural_decoder_trainer import loadModel\n",
    "import neuralDecoder.utils.lmDecoderUtils as lmDecoderUtils\n",
    "from neuralDecoder.utils.lmDecoderUtils import _cer_and_wer as cer_and_wer\n",
    "from models.lightning_wrapper import LightningWrapper\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from ipyfilechooser import FileChooser\n",
    "from tqdm import tqdm\n",
    "from edit_distance import SequenceMatcher\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseDir = root_directory = os.environ['DATA'] + '/willett2023'\n",
    "\n",
    "modelPath = baseDir + \"/rnn/speechBaseline4\"\n",
    "datsetPath = baseDir + \"/competitionData/pytorchTFRecords.pkl\"\n",
    "modelOutPath = baseDir + \"/competitionData/mamba\"\n",
    "\n",
    "model_checkpoint_path = FileChooser(baseDir + \"/experiments\")\n",
    "display(model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(modelPath + \"/args\", \"rb\") as handle:\n",
    "    args = pickle.load(handle)\n",
    "\n",
    "args[\"datasetPath\"] = datsetPath\n",
    "\n",
    "trainLoaders, testLoaders, loadedData = getDatasetLoaders(\n",
    "    args[\"datasetPath\"], args[\"batchSize\"]\n",
    ")\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "if model_checkpoint_path.selected:\n",
    "    model = LightningWrapper.load_from_checkpoint(model_checkpoint_path.selected)\n",
    "    #model = loadModel(modelPath, device=device)\n",
    "    model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cer(logits, X_len, y, y_len):\n",
    "    total_edit_distance = 0\n",
    "    total_seq_length = 0\n",
    "\n",
    "    adjustedLens = X_len\n",
    "    for iterIdx in range(logits.shape[0]):\n",
    "        decodedSeq = torch.argmax(\n",
    "            torch.tensor(logits[iterIdx, 0 : adjustedLens[iterIdx], :]),\n",
    "            dim=-1,\n",
    "        )  # [num_seq,]\n",
    "        decodedSeq = torch.unique_consecutive(decodedSeq, dim=-1)\n",
    "        decodedSeq = decodedSeq.cpu().detach().numpy()\n",
    "        decodedSeq = np.array([i for i in decodedSeq if i != 0])\n",
    "\n",
    "        trueSeq = np.array(\n",
    "            y[iterIdx][0 : y_len[iterIdx]].cpu().detach()\n",
    "        )\n",
    "\n",
    "        matcher = SequenceMatcher(\n",
    "            a=trueSeq.tolist(), b=decodedSeq.tolist()\n",
    "        )\n",
    "        total_edit_distance += matcher.distance()\n",
    "        total_seq_length += len(trueSeq)\n",
    "\n",
    "    return total_edit_distance / total_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run RNN Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_outputs(days, partition, loadedData, model, device):\n",
    "    model_outputs = {\n",
    "        \"logits\": [],\n",
    "        \"logitLengths\": [],\n",
    "        \"transcriptions\": [],\n",
    "        \"cer\": [],\n",
    "    }\n",
    "\n",
    "    for i, testDayIdx in enumerate(tqdm(days)):\n",
    "        # Competition data days do not correspond with the index\n",
    "        if partition == \"competition\":\n",
    "            test_ds = SpeechDataset([loadedData[partition][i]])\n",
    "        else:\n",
    "            test_ds = SpeechDataset([loadedData[partition][testDayIdx]])\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            test_ds, batch_size=1, shuffle=False, num_workers=0\n",
    "        )\n",
    "        for j, (X, y, X_len, y_len, day) in enumerate(test_loader):\n",
    "            X, y, X_len, y_len, dayIdx = (\n",
    "                X.to(device),\n",
    "                y.to(device),\n",
    "                X_len.to(device),\n",
    "                y_len.to(device),\n",
    "                torch.tensor([testDayIdx], dtype=torch.int64).to(device),\n",
    "            )\n",
    "            pred = model.forward(X, dayIdx)\n",
    "            adjustedLens = X_len #((X_len - model.kernelLen) / model.strideLen).to(torch.int32)\n",
    "\n",
    "            for iterIdx in range(pred.shape[0]):\n",
    "                model_outputs[\"logits\"].append(pred[iterIdx].cpu().detach().numpy())\n",
    "                model_outputs[\"logitLengths\"].append(\n",
    "                    adjustedLens[iterIdx].cpu().detach().item()\n",
    "                )\n",
    "\n",
    "            # Competition data days do not correspond with the index\n",
    "            if partition == \"competition\":\n",
    "                transcript = loadedData[partition][i][\"transcriptions\"][j]\n",
    "            else:\n",
    "                transcript = loadedData[partition][testDayIdx][\"transcriptions\"][j]\n",
    "\n",
    "            model_outputs[\"transcriptions\"].append(transcript)\n",
    "            model_outputs[\"cer\"].append(cer(pred, adjustedLens, y, y_len))\n",
    "\n",
    "    # Logits have different length\n",
    "    maxLogitLength = max([l.shape[0] for l in model_outputs['logits']])\n",
    "    model_outputs['logits'] = [np.pad(l, [[0, maxLogitLength-l.shape[0]], [0, 0]]) for l in model_outputs['logits']]\n",
    "    model_outputs['logits'] = np.stack(model_outputs['logits'], axis=0)\n",
    "    model_outputs['logitLengths'] = np.array(model_outputs['logitLengths'])\n",
    "    model_outputs['transcriptions'] = np.array(model_outputs['transcriptions'])\n",
    "    model_outputs['cer'] = np.array(model_outputs['cer'])\n",
    "\n",
    "    # Shift left all phonemes!!!\n",
    "    logits = model_outputs['logits']\n",
    "    model_outputs['logits'] = np.concatenate([logits[:, :, 1:], logits[:, :, :1]], axis=-1)\n",
    "\n",
    "    return model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test_outputs = get_model_outputs(days=range(4,19), partition=\"test\", loadedData=loadedData, model=model, device=device)\n",
    "print(\"Test raw CER: \", np.mean(model_test_outputs[\"cer\"]), flush=True)\n",
    "\n",
    "holdOutDays = [4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 18, 19, 20]\n",
    "model_holdOut_outputs = get_model_outputs(days=holdOutDays, partition=\"competition\", loadedData=loadedData, model=model, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save RNN Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputInfo(input):\n",
    "    for key in input.keys():\n",
    "        if type(input[key]) == np.ndarray:\n",
    "            print(key, input[key].shape, input[key].dtype, flush=True)\n",
    "        else:\n",
    "            print(key, type(input[key]), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out_path = modelOutPath + \"_test.pkl\"\n",
    "holdout_out_path = modelOutPath + \"_holdOut.pkl\"\n",
    "\n",
    "with open(test_out_path, \"wb\") as f:\n",
    "    pickle.dump(model_test_outputs, f)\n",
    "\n",
    "print(test_out_path + \" structure:\", flush=True)\n",
    "inputInfo(model_test_outputs)\n",
    "\n",
    "with open(holdout_out_path, \"wb\") as f:\n",
    "    pickle.dump(model_holdOut_outputs, f)\n",
    "\n",
    "print(holdout_out_path + \" structure:\", flush=True)\n",
    "inputInfo(model_holdOut_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the rnn outputs pkl for the LM\n",
    "with open(test_out_path, \"rb\") as handle:\n",
    "    model_test_outputs = pickle.load(handle)\n",
    "\n",
    "print(test_out_path + \" structure:\", flush=True)\n",
    "inputInfo(model_test_outputs)\n",
    "\n",
    "with open(holdout_out_path, \"rb\") as handle:\n",
    "    model_holdOut_outputs = pickle.load(handle)\n",
    "\n",
    "print(holdout_out_path + \" structure:\", flush=True)\n",
    "inputInfo(model_holdOut_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WER LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads the language model, could take a while and requires ~60 GB of memory\n",
    "lmDir = baseDir+'/languageModel'\n",
    "ngramDecoder = lmDecoderUtils.build_lm_decoder(\n",
    "    lmDir,\n",
    "    acoustic_scale=0.8, #1.2\n",
    "    nbest=1,\n",
    "    beam=18\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ngramDecoder, model_test_outputs, model_holdOut_outputs, outputFilePath):\n",
    "\n",
    "    print(\"\\nDecoding Test...\\n\", flush=True)\n",
    "    decoder_out_test = lmDecoderUtils.cer_with_lm_decoder(ngramDecoder, model_test_outputs, outputType='speech_sil', blankPenalty=np.log(2))\n",
    "\n",
    "    print(f\"\\n-------- WER: {decoder_out_test['wer']:.3f} --------\\n\", flush=True)\n",
    "\n",
    "    print(\"\\nDecoding HoldOut...\\n\", flush=True)\n",
    "    decoder_out_holdOut = lmDecoderUtils.cer_with_lm_decoder(ngramDecoder, model_holdOut_outputs, outputType='speech_sil', blankPenalty=np.log(2))\n",
    "    \n",
    "    filename = f\"{outputFilePath}_cer_{decoder_out_test['cer']:.3f}_wer_{decoder_out_test['wer']:.3f}.txt\"\n",
    "\n",
    "    print(\"\\nSaving \" + filename + \" ...\\n\", flush=True)\n",
    "    with open(filename, 'w') as f:\n",
    "        for decoded_transcript in decoder_out_holdOut['decoded_transcripts']:\n",
    "            f.write(decoded_transcript+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(ngramDecoder, model_test_outputs, model_holdOut_outputs, modelOutPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
